# pod for wiping ceph osd so you can rebuild

apiVersion: v1
kind: Pod
metadata:
  name: disk-wipe
  namespace: rook-ceph
spec:
  nodeName: duwerk-1
  restartPolicy: Never
  hostPID: true
  containers:
    - name: wipe
      image: quay.io/ceph/ceph:v19
      securityContext:
        privileged: true
      command:
        - sh
        - -c
        - |
          echo "Wiping /dev/nvme0n1..."
          sgdisk --zap-all /dev/nvme0n1
          dd if=/dev/zero of=/dev/nvme0n1 bs=1K count=200 oflag=direct,dsync seek=0
          dd if=/dev/zero of=/dev/nvme0n1 bs=1K count=200 oflag=direct,dsync seek=$((1 * 1024**2))
          dd if=/dev/zero of=/dev/nvme0n1 bs=1K count=200 oflag=direct,dsync seek=$((10 * 1024**2))
          dd if=/dev/zero of=/dev/nvme0n1 bs=1K count=200 oflag=direct,dsync seek=$((100 * 1024**2))
          blkdiscard /dev/nvme0n1
          partprobe /dev/nvme0n1
          echo "Done. Sleeping..."
          sleep 3600
      volumeMounts:
        - name: dev
          mountPath: /dev
  volumes:
    - name: dev
      hostPath:
        path: /dev